# -*- coding: utf-8 -*-
"""webflix112.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qz6rzqPAgOkCC0hq8gZlPFVToFsM8dS6
"""

import pandas as pd
url = 'https://drive.google.com/file/d/1fQ4E2q3LKng0ShMswV8kExZJtzfabn-P/view?usp=sharing' # movies.csv
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
movies_df = pd.read_csv(path)
url = 'https://drive.google.com/file/d/1cOW6sRWehqhw8936fFFuSgvkVxVlgyyh/view?usp=sharing' # ratings.csv
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
ratings_df = pd.read_csv(path)
url = 'https://drive.google.com/file/d/1aEracXXZf9HG5U-o_Mw3KX8aaFveiVvR/view?usp=sharing' # links.csv
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
links_df = pd.read_csv(path)
url = 'https://drive.google.com/file/d/1Fc9sf1t5yOR6rgtn-2-EMT5YKw5mReVI/view?usp=sharing' # tags.csv
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
tags_df = pd.read_csv(path)

movies_df

tags_df

ratings_df

links_df

"""# **movies**"""

movies_df.shape

movies_df.info()

movies_df.describe()

movies_df.head()

movies_df["movieId"].nunique()

movies_df["title"].nunique()

movies_df.isnull().sum()

movies_df["genres"].nunique()

"""# **tags**"""

tags_df.head()

tags_df["tag"].nunique()

tags_df["timestamp"].nunique()

tags_df["userId"].nunique()

tags_df.shape

tags_df.info()

tags_df.describe()

tags_df.isnull().sum()

"""# **links**"""

links_df.head()

links_df.shape

links_df.info()

links_df.describe()

links_df.isnull().sum()

"""# **rating**"""

ratings_df.head()

ratings_df["rating"].nunique()

ratings_df.shape

ratings_df.isnull().sum()

ratings_df.info()

ratings_df.describe()

"""# **merged rating and movies**"""

df = pd.merge(ratings_df, movies_df, on='movieId')
df

df.isna().any()

df.isnull().sum()

"""# **popularity rankings**"""

rating_count_df = df.groupby('title')['rating'].agg(['mean', 'count']).reset_index()
rating_count_df.nlargest(5, ['mean', 'count'])

average_ratings = df.groupby('title')['rating'].mean().reset_index()
average_ratings

popularity_rankings = average_ratings.sort_values(by='rating', ascending= False)
popularity_rankings

"""# **highest rated movies**"""

highest_rated_title = popularity_rankings.iloc[0]['title']
highest_rated_movie_mask = popularity_rankings['title'] == highest_rated_title

movie_info_columns = ['title','rating']
highest_rated_movie_info = popularity_rankings.loc[highest_rated_movie_mask, movie_info_columns]

print(highest_rated_movie_info)

"""# **most rated movies**"""

rating_count_df.sort_values(by=['count', 'mean'], ascending=False).head()

rating_counts = df['movieId'].value_counts().reset_index()
rating_counts.columns = ['movieId', 'count']
rating_counts.columns

most_rated_movieId = rating_counts['movieId'].iloc[0]
most_rated_movie_mask = df['movieId'] == most_rated_movieId
movie_info_columns = ['movieId', 'title', 'genres']
most_rated_movie_info = df.loc[most_rated_movie_mask, movie_info_columns].drop_duplicates()
print(most_rated_movie_info)

most_rated_movieId = df['movieId'].value_counts().idxmax()
most_rated_movie_title = df[df['movieId'] == most_rated_movieId]['title'].values[0]
print("Most Rated Movie:", most_rated_movie_title)

import pandas as pd
import matplotlib.pyplot as plt

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
movie_counts = pd.merge(rating_counts, movies_df[['movieId', 'title']], on='movieId', how='left')
movie_counts = movie_counts.sort_values(by='count', ascending=False)
plt.figure(figsize=(12, 6))
sns.barplot(x='count', y='title', data=movie_counts.head(20))  # Display the top 20 most rated movies
plt.title('Top 20 Most Rated Movies')
plt.xlabel('Number of Ratings')
plt.ylabel('Movie Title')
plt.show()

"""# **Computing similarities**"""

from pandas import DataFrame

user_df = DataFrame(
    {'user_x': [1, 3, 2, 4],
     'user_y': [1, 4, 3, 4],
     'user_z': [1, 5, 5, 1]},
    index=['movie_A', 'movie_B', 'movie_C', 'movie_D']
)

from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(user_df.T)

user_similarity_matrix = cosine_similarity(user_df)
user_similarity_matrix

user_df

user_similarity_matrix

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(user_similarity_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('User Similarity Heatmap')
plt.show()

"""# **Item based**"""

df

user_movie_matrix = pd.pivot_table(data=df,
                                  values='rating',
                                  index='userId',
                                  columns='movieId',
                                  fill_value=0)
user_movie_matrix.head(10)

"""# Pearson correlation matrix"""

movie_correlations_matrix = user_movie_matrix.corr()
movie_correlations_matrix

def popularity_recommender():
    n = int(input('Enter the number of movies you want to recommend: '))

    movie_stats = ratings_df.groupby('movieId').agg({'rating': ['mean', 'count']}).reset_index()

    # Filter movies with a mean rating greater than 4 and at least 100 ratings
    popular_movies = movie_stats[(movie_stats['rating']['mean'] > 4) & (movie_stats['rating']['count'] > 100)]

    # Sort popular movies by mean rating in descending order
    popular_movies = popular_movies.sort_values(by=[('rating', 'mean')], ascending=False)

    # Merge with movie information
    popular_movies = pd.merge(popular_movies, movies_df, on='movieId', how='left')

    # Return the top n popular movies
    return popular_movies.head(n)

# Call the function and display the recommended movies
recommended_movies = popularity_recommender()
print(recommended_movies)

"""# **Most popular movie**"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
# Step 1: Find the most popular movie
most_popular_movie = df.groupby('movieId')['rating'].mean().idxmax()

user_movie_matrix = df.pivot_table(index='userId', columns='movieId', values='rating', fill_value=0)
#cosine similarity
movie_similarity_scores = cosine_similarity(user_movie_matrix.T)  # Transpose for movie-wise similarity
#Find movies similar to the most popular movie
similar_movies_indices = np.argsort(movie_similarity_scores[most_popular_movie])[::-1]
similar_movie_ids = user_movie_matrix.columns[similar_movies_indices]
N = 10
similar_movies = movies_df[movies_df['movieId'].isin(similar_movie_ids[:N])]
# Display the top N similar movies
similar_movies.head(N)

most_popular_movie

import numpy as np
import matplotlib.pyplot as plt

# Assuming you have already calculated movie_similarity_scores as described before

# Choose a movie to find similar movies for
movie_to_find_similarities = "Indian in the Cupboard, The (1995)"

# Find the index of the movie in your dataset (assuming a movies_df DataFrame)
movie_index = movies_df[movies_df['title'] == movie_to_find_similarities].index[0]

# Sort the movies by similarity score
similar_movies_indices = np.argsort(movie_similarity_scores[movie_index])[::-1]

# Number of similar movies to display
N = 10

# Get the top N most similar movies
top_similar_movies_indices = similar_movies_indices[1:N+1]  # Exclude the movie itself
top_similar_movie_scores = movie_similarity_scores[movie_index][top_similar_movies_indices]
top_similar_movie_titles = movies_df.iloc[top_similar_movies_indices]['title']

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_similar_movie_titles, top_similar_movie_scores, color='skyblue')
plt.xlabel('Similarity Score')
plt.title(f'Top {N} Movies Similar to "{movie_to_find_similarities}"')
plt.gca().invert_yaxis()
plt.show()

"""# **User based recommender**"""



from surprise import Dataset, Reader, KNNBasic
from surprise.model_selection import train_test_split
from surprise import accuracy

data = df[['userId','movieId','title','rating']]

from surprise import Dataset, Reader
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)

# Split the data into a training set and a test set (80% train, 20% test)
trainset, testset = train_test_split(data, test_size=0.2, random_state=142)

"""## Making a model to test"""

sim_options = {
    'name': 'cosine',
    'user_based': True
}

knn = KNNBasic(sim_options=sim_options)

knn.fit(trainset)

predictions = knn.test(testset)

"""## Exploring the predictions"""

predictions[:5]

predictions_df = pd.DataFrame(predictions, columns=["raw_user_id", "raw_item_id", "actual_user_rating", "estimated_user_rating", "details"])
predictions_df.head(10)

mae = accuracy.mae(predictions)    # mean absolute error
rmse = accuracy.rmse(predictions) #Root Mean Square Error

print(f"MAE: {mae}")
print(f"RMSE: {rmse}")

accuracy.fcp(predictions) #Fraction of Concordant Pairs

"""manual approach

## **Histogram**
"""

predictions_df["difference"] = predictions_df["actual_user_rating"] - predictions_df["estimated_user_rating"]

predictions_df["difference"].hist(bins=30,
                                  figsize=(10, 8));

"""## **Boxplot of errors**"""

predictions_df.plot(kind='box',
                    column='estimated_user_rating',
                    by='actual_user_rating',
                    figsize=(10, 8));

"""##  Making a model to use"""

sim_options = {
    'name': 'cosine',
    'user_based': True
}

full_train = data.build_full_trainset()
algo = KNNBasic(sim_options=sim_options)
algo.fit(trainset)

testset = trainset.build_anti_testset()
predictions = algo.test(testset)

def get_top_n(predictions, user_id, n=10):

  user_recommendations = []

  # Iterate through each prediction tuple
  for uid, iid, true_r, est, _ in predictions:
    # Check if the user ID matches the target user
    if user_id == uid:
      # Append item_id and estimated_rating to the user_recommendations list
      user_recommendations.append((iid, est))
    else:
      # Skip to the next prediction if user ID doesn't match
      continue

  # Sort the user_recommendations list based on estimated_rating in descending order
  ordered_recommendations = sorted(user_recommendations, key=lambda x: x[1], reverse=True)

  # Get the top n predictions from the ordered_recommendations
  ordered_recommendations_top_n = ordered_recommendations[:n]

  return ordered_recommendations_top_n

user_id = 15
n = 20

top_n = get_top_n(predictions, user_id, n)
top_n

# Creating a DataFrame from the top_n tuples with columns 'movieId' and 'estimated_rating'
tuples_df = pd.DataFrame(top_n, columns=["movieId", "estimated_rating"])

# Creating a copy of the original DataFrame with duplicate 'movieId' entries removed
reduced_df = movies_df.drop_duplicates(subset='movieId').copy()

# Merging the tuples_df with the reduced_df based on 'movieId', retaining only the matching rows
tuples_df_expanded = tuples_df.merge(reduced_df, on="movieId", how='left')

# Selecting specific columns from the merged DataFrame to include in the final result
tuples_df_expanded = tuples_df_expanded[['movieId', 'title', 'genres']]

# Displaying the expanded DataFrame with relevant movie information
tuples_df_expanded

df


